{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECN Spider data input\n",
    "\n",
    "First, load prerequisites and configure things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bz2\n",
    "from ipaddress import ip_address\n",
    "\n",
    "# Change this to point at the raw data files\n",
    "\n",
    "DATAPATH=\".\"\n",
    "TABLESDIR=\".\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in data from ecnspider from each vantage point and massage it into a useful format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 540 ms, total: 11.4 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "def load_es_df(filename, vp, trial):\n",
    "    # raw dataframe\n",
    "    df = pd.read_csv(filename, names=[\"time\", \"rank\", \"site\", \"ip\",\n",
    "                                      \"ecn0rv\",\"ecn0sp\",\"ecn1rv\",\"ecn1sp\",\n",
    "                                      \"ecn0http\",\"ecn1http\"],\n",
    "                     usecols=[0,1,2,3,4,5,6,7,16,19])\n",
    "\n",
    "    # cast IP address to string\n",
    "    df['ip'] = df['ip'].astype(np.str)\n",
    "\n",
    "    # drop all rows with garbage addresses\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"[::\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"[fe80:\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"[fc00:\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"[64:ff9b:\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"0.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"10.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"127.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"169.254.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.16.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.17.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.18.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.19.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.20.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.21.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.22.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.23.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.24.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.25.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.26.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.27.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.28.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.29.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.30.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"172.31.\"))]\n",
    "    df = df[df['ip'].apply(lambda x: not x.startswith(\"192.168.\"))]\n",
    "    \n",
    "    # tag IPv6 addresses\n",
    "    df[\"ip6\"] = df['ip'].apply(lambda x: x.startswith(\"[\"))\n",
    "\n",
    "    # cast timestamp to datetime\n",
    "    df['time'] = pd.to_datetime(df['time'] * 1e9)\n",
    "\n",
    "    # rank is an integer\n",
    "    df['rank'] = df['rank'].astype(np.uint32)\n",
    "\n",
    "    # sitr is a string\n",
    "    df['site'] = df['site'].astype(np.str)\n",
    "\n",
    "    # cast ports\n",
    "    df[\"ecn0sp\"] = df[\"ecn0sp\"].astype(np.uint16)\n",
    "    df[\"ecn1sp\"] = df[\"ecn1sp\"].astype(np.uint16)\n",
    "\n",
    "    # categorize errors\n",
    "    df[\"ecn0rv\"] = pd.Categorical(df['ecn0rv'].fillna(\"Success\"))\n",
    "    df[\"ecn1rv\"] = pd.Categorical(df['ecn1rv'].fillna(\"Success\"))\n",
    "    df[\"ecn0ok\"] = (df['ecn0rv'] == \"Success\")\n",
    "    df[\"ecn1ok\"] = (df['ecn1rv'] == \"Success\")\n",
    "\n",
    "    # cast HTTP status\n",
    "    df[\"ecn0http\"] = df[\"ecn0http\"].fillna(0).astype(np.uint16)\n",
    "    df[\"ecn1http\"] = df[\"ecn1http\"].fillna(0).astype(np.uint16)\n",
    "\n",
    "    # annotate mismatch between error states\n",
    "    # (the error codes are less interesting; the fact that the status is different moreso)\n",
    "    df[\"ecndep\"] = (df[\"ecn0ok\"] != df[\"ecn1ok\"])\n",
    "\n",
    "    # annotate with vp and trial, in case we want pivot/select on these later\n",
    "    df[\"vp\"] = vp\n",
    "    df[\"trial\"] = trial\n",
    "    \n",
    "    # and now build the index\n",
    "    df.index = pd.Index(df['ip'], name=\"ip\")\n",
    "    del(df['ip'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "es_df = {}\n",
    "%time es_df[\"ams-0\"] = load_es_df(DATAPATH+\"/results0.csv\", \"ams\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Merging\n",
    "\n",
    "Now pull all the ECN Spider data from all runs into a single dataframe on which we can do subsequent column comparisons to look at temporal and spatial dependency of ECN connectivity dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def index_intersect(dfs):\n",
    "    \"\"\"Return the intersection of the indices of passed-in dataframes\"\"\"\n",
    "    idx = dfs[0].index\n",
    "    for i in range(1, len(dfs)):\n",
    "        idx = idx & dfs[i].index\n",
    "    return pd.Index(idx.unique(), name=dfs[0].index.name)\n",
    "\n",
    "def connmatrix(es_dfs, vps, trials):\n",
    "    # use only items in every dataframe\n",
    "    idx = index_intersect(es_dfs)\n",
    "    \n",
    "    # make an initial dataframe from the first\n",
    "    e0col = \"-\".join([str(vps[0]),str(trials[0]),\"e0\"])\n",
    "    e0cols = [e0col]\n",
    "    e1col = \"-\".join([str(vps[0]),str(trials[0]),\"e1\"])\n",
    "    e1cols = [e1col]\n",
    "#     eqcol = \"-\".join([str(vps[0]),str(trials[0]),\"eq\"])\n",
    "#     eqcols = [eqcol]\n",
    "#     depcol = \"-\".join([str(vps[0]),str(trials[0]),\"dep\"])\n",
    "#     depcols = [depcol]\n",
    "#     oddcol = \"-\".join([str(vps[0]),str(trials[0]),\"odd\"])\n",
    "#     oddcols = [oddcol]\n",
    "    cat_df = es_dfs[0].loc[idx, [\"rank\", \"site\", \"ip6\", \"ecn0ok\", \"ecn1ok\"]]\n",
    "    cat_df.columns = [\"rank\", \"site\", \"ip6\", e0col, e1col]\n",
    "#     cat_df[eqcol] = (cat_df[e0col] & cat_df[e1col]) | (~cat_df[e0col] & ~cat_df[e1col])\n",
    "#     cat_df[depcol] = cat_df[e0col] & ~cat_df[e1col]\n",
    "#     cat_df[oddcol] = ~cat_df[e0col] & cat_df[e1col]\n",
    "\n",
    "    # now add columns to the catdf\n",
    "    for i in range(1, len(es_dfs)):\n",
    "        e0col = \"-\".join([str(vps[i]),str(trials[i]),\"e0\"])\n",
    "        e0cols += [e0col]\n",
    "        e1col = \"-\".join([str(vps[i]),str(trials[i]),\"e1\"])\n",
    "        e1cols += [e1col]\n",
    "#         eqcol = \"-\".join([str(vps[i]),str(trials[i]),\"eq\"])\n",
    "#         eqcols += [eqcol]\n",
    "#         depcol = \"-\".join([str(vps[i]),str(trials[i]),\"dep\"])\n",
    "#         depcols += [depcol]       \n",
    "#         oddcol = \"-\".join([str(vps[i]),str(trials[i]),\"odd\"])\n",
    "#         oddcols += [oddcol]        \n",
    "        cat_df[e0col] = es_dfs[i].loc[idx][\"ecn0ok\"]\n",
    "        cat_df[e1col] = es_dfs[i].loc[idx][\"ecn1ok\"]\n",
    "#         cat_df[eqcol] = (cat_df[e0col] & cat_df[e1col]) | (~cat_df[e0col] & ~cat_df[e1col])\n",
    "#         cat_df[depcol] = cat_df[e0col] & ~cat_df[e1col]\n",
    "#         cat_df[oddcol] = ~cat_df[e0col] & cat_df[e1col]\n",
    "\n",
    "#     # add a few columns summarizing all\n",
    "#     # all eq = no evidence of ECN dependency\n",
    "#     cat_df[\"all-eq\"] = cat_df.loc[:,eqcols].all(axis=1)\n",
    "#     # count of equal trials\n",
    "#     cat_df[\"eq-sum\"] = cat_df.loc[:,eqcols].sum(axis=1)\n",
    "#     # count of e0 connections\n",
    "#     cat_df[\"e0-sum\"] = cat_df.loc[:,e0cols].sum(axis=1)\n",
    "#     # count of e1 connections\n",
    "#     cat_df[\"e1-sum\"] = cat_df.loc[:,e1cols].sum(axis=1)\n",
    "#     # count of odd connections\n",
    "#     cat_df[\"dep-sum\"] = cat_df.loc[:,depcols].sum(axis=1)\n",
    "#     # count of odd connections\n",
    "#     cat_df[\"odd-sum\"] = cat_df.loc[:,oddcols].sum(axis=1)\n",
    "#     # all conn = no connection failure at all\n",
    "#     cat_df[\"all-conn\"] = cat_df.loc[:,e0cols+e1cols].all(axis=1)\n",
    "#     # no conn = permanent connection failure\n",
    "#     cat_df[\"no-conn\"] = ~cat_df.loc[:,e0cols+e1cols].any(axis=1)\n",
    "\n",
    "    return cat_df\n",
    "\n",
    "cc_df = connmatrix([es_df[\"ams-0\"]], [\"ams\"], [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QoF flow data input\n",
    "\n",
    "Now we load IPFIX flow data generated by QoF. We'll link this to the ECN spider connectivity data by destination address.\n",
    "\n",
    "Because python-ipfix is relatively slow, we can either reload from a pre-loaded HDF5 table or direct from raw IPFIX; both possibilities are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipfix\n",
    "import panfix\n",
    "\n",
    "# configure IPFIX information model\n",
    "ipfix.ie.use_iana_default()\n",
    "ipfix.ie.use_5103_default()\n",
    "ipfix.ie.use_specfile(\"qof.iespec\")\n",
    "\n",
    "# Define flags\n",
    "S = panfix.TCP_SYN\n",
    "R = panfix.TCP_RST\n",
    "SA = panfix.TCP_SYN | panfix.TCP_ACK\n",
    "SEW = (panfix.TCP_SYN | panfix.TCP_ECE | panfix.TCP_CWR)\n",
    "SAE = (panfix.TCP_SYN | panfix.TCP_ECE | panfix.TCP_ACK)\n",
    "SAEW = (panfix.TCP_SYN | panfix.TCP_ECE | panfix.TCP_ACK | panfix.TCP_CWR)\n",
    "QECT = (panfix.QOF_ECT0 | panfix.QOF_ECT1)\n",
    "QECT0 = panfix.QOF_ECT0\n",
    "QECT1 = panfix.QOF_ECT1\n",
    "QCE = panfix.QOF_CE\n",
    "\n",
    "# iain's last syn qof characteristics flags\n",
    "QSYNECT0 = 0x0100\n",
    "QSYNECT1 = 0x0200\n",
    "QSYNCE   = 0x0400\n",
    "\n",
    "def load_qof_df(filename, ipv6_mode=False, open_fn=open, spider_idx=None, count=None):\n",
    "    # select destination address IE\n",
    "    if ipv6_mode:\n",
    "        dip_ie = \"destinationIPv6Address\"\n",
    "    else:\n",
    "        dip_ie = \"destinationIPv4Address\"\n",
    "    \n",
    "    # raw dataframe\n",
    "    df = panfix.dataframe_from_ipfix(filename, open_fn=open_fn, count=count,\n",
    "               ienames=(  \"flowStartMilliseconds\",\n",
    "                          \"octetDeltaCount\",\n",
    "                          \"reverseOctetDeltaCount\",\n",
    "                          \"transportOctetDeltaCount\",\n",
    "                          \"reverseTransportOctetDeltaCount\",\n",
    "                          \"tcpSequenceCount\",\n",
    "                          \"reverseTcpSequenceCount\",\n",
    "                          dip_ie,\n",
    "                          \"sourceTransportPort\",\n",
    "                          \"destinationTransportPort\",\n",
    "                          \"initialTCPFlags\",\n",
    "                          \"reverseInitialTCPFlags\",\n",
    "                          \"unionTCPFlags\",\n",
    "                          \"reverseUnionTCPFlags\",\n",
    "                          \"lastSynTcpFlags\",\n",
    "                          \"reverseLastSynTcpFlags\",\n",
    "                          \"tcpSynTotalCount\",\n",
    "                          \"reverseTcpSynTotalCount\",\n",
    "                          \"qofTcpCharacteristics\",\n",
    "                          \"reverseQofTcpCharacteristics\",\n",
    "                          \"reverseMinimumTTL\",\n",
    "                          \"reverseMaximumTTL\"))\n",
    "\n",
    "    # turn timestamps into pandas-friendly types\n",
    "    df = panfix.coerce_timestamps(df)\n",
    "    \n",
    "    # cast flags down to reduce memory consumption\n",
    "    df[\"initialTCPFlags\"] = df[\"initialTCPFlags\"].astype(np.uint8)\n",
    "    df[\"reverseInitialTCPFlags\"] = df[\"reverseInitialTCPFlags\"].astype(np.uint8)\n",
    "    df[\"unionTCPFlags\"] = df[\"unionTCPFlags\"].astype(np.uint8)\n",
    "    df[\"reverseUnionTCPFlags\"] = df[\"reverseUnionTCPFlags\"].astype(np.uint8)\n",
    "    df[\"lastSynTcpFlags\"] = df[\"lastSynTcpFlags\"].astype(np.uint8)\n",
    "    df[\"reverseLastSynTcpFlags\"] = df[\"reverseLastSynTcpFlags\"].astype(np.uint8)\n",
    "    \n",
    "    # drop all flows without dport == 80\n",
    "    df = df[df[\"destinationTransportPort\"] == 80]\n",
    "    del(df[\"destinationTransportPort\"])\n",
    "    \n",
    "    # drop all flows without an initial SYN\n",
    "    df = df[np.bitwise_and(df[\"initialTCPFlags\"], S) > 0]\n",
    "        \n",
    "    # cast addresses to strings to match ecnspider data\n",
    "    if ipv6_mode:\n",
    "        df[dip_ie] = df[dip_ie].apply(lambda x: \"[\"+str(x)+\"]\")\n",
    "    else:\n",
    "        df[dip_ie] = df[dip_ie].apply(str)\n",
    "\n",
    "    # mark IPv6 mode\n",
    "    df['ip6'] = ipv6_mode\n",
    "        \n",
    "    # now build the index\n",
    "    df.index = pd.Index(df[dip_ie], name=\"ip\")\n",
    "    del(df[dip_ie])\n",
    "\n",
    "    # filter on index if requested\n",
    "    if spider_idx is not None:\n",
    "        qof_idx = pd.Index((spider_idx & df.index).unique(), name=spider_idx.name)\n",
    "        df = df.loc[qof_idx]\n",
    "\n",
    "    # Now annotate the dataframe with ECN and establishment columns\n",
    "    df[\"ecnAttempted\"] = np.bitwise_and(df[\"lastSynTcpFlags\"],SAEW) == SEW\n",
    "    df[\"ecnNegotiated\"] = np.bitwise_and(df[\"reverseLastSynTcpFlags\"],SAEW) == SAE\n",
    "    df[\"ecnCapable\"] = np.bitwise_and(df[\"reverseQofTcpCharacteristics\"],QECT0) > 0\n",
    "    df[\"ecnECT1\"] = np.bitwise_and(df[\"reverseQofTcpCharacteristics\"],QECT1) > 0\n",
    "    df[\"ecnCE\"] = np.bitwise_and(df[\"reverseQofTcpCharacteristics\"],QCE) > 0\n",
    "    df[\"didEstablish\"] = ((np.bitwise_and(df[\"lastSynTcpFlags\"], S) == S) &\n",
    "                          (np.bitwise_and(df[\"reverseLastSynTcpFlags\"], SA) == SA))\n",
    "    df[\"isUniflow\"] = (df[\"reverseMaximumTTL\"] == 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reload flow dataframes from HDF5, or...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.12 s, sys: 700 ms, total: 4.82 s\n",
      "Wall time: 4.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load from HDF5\n",
    "qof4_df = {}\n",
    "qof6_df = {}\n",
    "with pd.get_store(DATAPATH+\"/ecnqof.hdf5\") as qofstore:\n",
    "    for key in qofstore.keys():\n",
    "        if key.startswith(\"/\"): \n",
    "            key = key[1:]\n",
    "        (vp, trial, ipv) = key.split(\"-\")\n",
    "        if ipv == \"ip4\":\n",
    "            qof4_df[vp+\"-\"+trial] = qofstore[key]\n",
    "        elif ipv == \"ip6\":\n",
    "            qof6_df[vp+\"-\"+trial] = qofstore[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...from IPFIX: the latter is much slower, necessary when we have rerun QoF, otherwise skippable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 3.38 s, total: 1min 45s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "# (re)load IPv4 QoF dataframes\n",
    "qof4_df = {}\n",
    "%time qof4_df['ams-0'] = load_qof_df(filename=DATAPATH+\"/results0.ipfix\", spider_idx=cc_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.72 s, sys: 147 ms, total: 3.87 s\n",
      "Wall time: 3.87 s\n"
     ]
    }
   ],
   "source": [
    "# (re)load IPv6 QoF dataframes\n",
    "qof6_df = {}\n",
    "%time qof6_df['ams-0'] = load_qof_df(filename=DATAPATH+\"/results0.ipfix\", spider_idx=cc_df.index, ipv6_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/tables/path.py:100: NaturalNameWarning: object name is not a valid Python identifier: 'ams-0-ip4'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  NaturalNameWarning)\n",
      "/usr/local/lib/python3.5/site-packages/tables/path.py:100: NaturalNameWarning: object name is not a valid Python identifier: 'ams-0-ip6'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  NaturalNameWarning)\n"
     ]
    }
   ],
   "source": [
    "# and store to HDF5 so we can retrieve them later\n",
    "with pd.get_store(DATAPATH+\"/ecnqof.hdf5\") as qofstore:\n",
    "    for key in qof4_df.keys():\n",
    "        qofstore[key+\"-ip4\"] = qof4_df[key]\n",
    "    for key in qof6_df.keys():\n",
    "        qofstore[key+\"-ip6\"] = qof6_df[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the QoF dataframes based on whether an ECN attempt was seen or not, and merge these back together as with the ECN Spider data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_qof_df(df):\n",
    "    # split on attempt\n",
    "    qe0_df = df[~df['ecnAttempted']]\n",
    "    qe1_df = df[ df['ecnAttempted']]\n",
    "    \n",
    "    # take only the biggest object HACK HACK HACK\n",
    "    qe0_df = qe0_df.sort(\"reverseTransportOctetDeltaCount\",ascending=False).groupby(level=0).first()  \n",
    "    qe1_df = qe1_df.sort(\"reverseTransportOctetDeltaCount\",ascending=False).groupby(level=0).first()\n",
    "\n",
    "    # take only rows appearing in both\n",
    "    qof_idx = index_intersect([qe0_df, qe1_df])\n",
    "    qe0_df = qe0_df.loc[qof_idx]\n",
    "    qe1_df = qe1_df.loc[qof_idx]\n",
    "    \n",
    "    return (qe0_df, qe1_df)\n",
    "    \n",
    "def flowmatrix_columns(mq_df):\n",
    "    return filter(lambda x: x != \"ip6\", mq_df.columns)\n",
    "    \n",
    "def flowmatrix(qof_dfs, labels):\n",
    "    mq_dfs = []\n",
    "    for qof_df in qof_dfs:\n",
    "        # split on ecn attempted\n",
    "        (qe0_df, qe1_df) = split_qof_df(qof_df)\n",
    "        \n",
    "        # and merge back together\n",
    "        mqof_df = qe0_df.loc[:,[\"ip6\", \"didEstablish\", \"ecnCapable\", \"ecnECT1\", \"ecnCE\",\n",
    "                                \"lastSynTcpFlags\", \"reverseLastSynTcpFlags\", \n",
    "                                \"reverseUnionTCPFlags\", \"reverseMaximumTTL\"]]\n",
    "        mqof_df.columns = [\"ip6\", \"e0\", \"e0ect0\", \"e0ect1\", \"e0ce\", \n",
    "                           \"e0f\", \"e0rf\", \"e0ruf\", \"ttl\"]\n",
    "        mqof_df[\"z0\"] = (qe0_df[\"reverseTransportOctetDeltaCount\"] == 0)\n",
    "        mqof_df[\"z1\"] = (qe1_df[\"reverseTransportOctetDeltaCount\"] == 0)\n",
    "        mqof_df[\"e1\"] = qe1_df[\"didEstablish\"]\n",
    "        mqof_df[\"neg\"] = qe1_df[\"ecnNegotiated\"]\n",
    "        # markings on ECN negotiated flows\n",
    "        mqof_df[\"ect0\"] = qe1_df[\"ecnCapable\"]\n",
    "        mqof_df[\"ect1\"] = qe1_df[\"ecnECT1\"]\n",
    "        mqof_df[\"ce\"] = qe1_df[\"ecnCE\"]\n",
    "        mqof_df[\"synect0\"] = np.bitwise_and(qe1_df[\"reverseQofTcpCharacteristics\"], QSYNECT0) == QSYNECT0\n",
    "        mqof_df[\"synect1\"] = np.bitwise_and(qe1_df[\"reverseQofTcpCharacteristics\"], QSYNECT1) == QSYNECT1\n",
    "        mqof_df[\"synce\"] = np.bitwise_and(qe1_df[\"reverseQofTcpCharacteristics\"], QSYNCE) == QSYNCE\n",
    "        # markings on non-negotiated flows\n",
    "        mqof_df[\"e0ect0\"] =    qe0_df[\"ecnCapable\"]\n",
    "        mqof_df[\"e0ect1\"] =    qe0_df[\"ecnECT1\"]\n",
    "        mqof_df[\"e0ce\"] =      qe0_df[\"ecnCE\"]\n",
    "        mqof_df[\"e0synect0\"] = np.bitwise_and(qe0_df[\"reverseQofTcpCharacteristics\"], QSYNECT0) == QSYNECT0\n",
    "        mqof_df[\"e0synect1\"] = np.bitwise_and(qe0_df[\"reverseQofTcpCharacteristics\"], QSYNECT1) == QSYNECT1\n",
    "        mqof_df[\"e0synce\"] =   np.bitwise_and(qe0_df[\"reverseQofTcpCharacteristics\"], QSYNCE) == QSYNCE\n",
    "\n",
    "        mqof_df[\"refl\"] = np.bitwise_and(qe1_df[\"reverseLastSynTcpFlags\"], SAEW) == SAEW\n",
    "        \n",
    "        # add to list of merged dataframes\n",
    "        mq_dfs.append(mqof_df)\n",
    "    \n",
    "    # use only items in every dataframe\n",
    "    idx = index_intersect(mq_dfs)\n",
    "    \n",
    "    # make an initial dataframe from the first\n",
    "    pfx = labels[0]+\"-\"\n",
    "    cat_df = mq_dfs[0].loc[idx]\n",
    "    cat_df.columns = [\"ip6\"] + [pfx + col for col in flowmatrix_columns(mq_dfs[0])]\n",
    "    cat_df[pfx+\"ect\"] = cat_df[pfx+\"ect0\"] | cat_df[pfx+\"ect1\"]\n",
    "    cat_df[pfx+\"negok\"] = cat_df[pfx+\"neg\"] & cat_df[pfx+\"ect\"]\n",
    "    \n",
    "    # now add columns to the catdf\n",
    "    for i in range(1, len(mq_dfs)):\n",
    "        pfx = labels[i]+\"-\"\n",
    "        for col in flowmatrix_columns(mq_dfs[i]):\n",
    "            cat_df[pfx+col] = mq_dfs[i].loc[idx][col]\n",
    "        cat_df[pfx+\"ect\"] = cat_df[pfx+\"ect0\"] | cat_df[pfx+\"ect1\"]\n",
    "        cat_df[pfx+\"negok\"] = cat_df[pfx+\"neg\"] & cat_df[pfx+\"ect\"]\n",
    "\n",
    "        \n",
    "    # now some sums\n",
    "    sum_cols = [\"negok\",\"neg\",\"ect\",\"refl\",\n",
    "                \"ect0\",\"ect1\",\"ce\",\n",
    "                \"synect0\",\"synect1\",\"synce\",\n",
    "                \"e0ect0\",\"e0ect1\",\"e0ce\",\n",
    "                \"e0synect0\",\"e0synect1\",\"e0synce\",\n",
    "                \"e1\",\"e0\",\"z1\",\"z0\"]\n",
    "    for sum_col in sum_cols:\n",
    "        cat_df[sum_col+\"-sum\"] = cat_df.loc[:,[label+\"-\"+sum_col for label in labels]].sum(axis=1)\n",
    "\n",
    "    return cat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:7: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:8: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 s, sys: 1.68 s, total: 13 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "vps    = [\"ams\"]\n",
    "trials = [str(x) for x in [0]]\n",
    "labels = [\"-\".join(l) for l in itertools.product(vps,trials)]\n",
    "\n",
    "qq4_df = flowmatrix([qof4_df[label] for label in labels],\n",
    "                   labels)\n",
    "qq4_df[\"rank\"] = cc_df.loc[qq4_df.index][\"rank\"]\n",
    "qq4_df[\"site\"] = cc_df.loc[qq4_df.index][\"site\"]\n",
    "\n",
    "\n",
    "qq6_df = flowmatrix([qof6_df[label] for label in labels],\n",
    "                     labels)\n",
    "qq6_df[\"rank\"] = cc_df.loc[qq6_df.index][\"rank\"]\n",
    "qq6_df[\"site\"] = cc_df.loc[qq6_df.index][\"site\"]\n",
    "\n",
    "qq_df = pd.concat((qq4_df, qq6_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rank', 'site', 'ip6', 'ams-0-e0', 'ams-0-e1'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ip6', 'ams-0-e0', 'ams-0-e0ect0', 'ams-0-e0ect1', 'ams-0-e0ce',\n",
       "       'ams-0-e0f', 'ams-0-e0rf', 'ams-0-e0ruf', 'ams-0-ttl', 'ams-0-z0',\n",
       "       'ams-0-z1', 'ams-0-e1', 'ams-0-neg', 'ams-0-ect0', 'ams-0-ect1',\n",
       "       'ams-0-ce', 'ams-0-synect0', 'ams-0-synect1', 'ams-0-synce',\n",
       "       'ams-0-e0synect0', 'ams-0-e0synect1', 'ams-0-e0synce', 'ams-0-refl',\n",
       "       'ams-0-ect', 'ams-0-negok', 'negok-sum', 'neg-sum', 'ect-sum',\n",
       "       'refl-sum', 'ect0-sum', 'ect1-sum', 'ce-sum', 'synect0-sum',\n",
       "       'synect1-sum', 'synce-sum', 'e0ect0-sum', 'e0ect1-sum', 'e0ce-sum',\n",
       "       'e0synect0-sum', 'e0synect1-sum', 'e0synce-sum', 'e1-sum', 'e0-sum',\n",
       "       'z1-sum', 'z0-sum', 'rank', 'site'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe CSV output\n",
    "\n",
    "Output the `cc_df` table, which has the following columns:\n",
    "\n",
    "- `ip`: index, IP address as string\n",
    "- `rank`: Alexa website rank\n",
    "- `site`: Website hostname\n",
    "- `ip6`: True if address is IPv6 (for convenience in splitting v4 and v6 analyses without string munging)\n",
    "- *vantage*`-`*trial*`-e0`: True if connection succeeded for *trial* at *vantage* without ECN negotiation attempt\n",
    "- *vantage*`-`*trial*`-e1`: True if connection succeeded for *trial* at *vantage* with ECN negotiation attempt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc_df.to_csv(TABLESDIR+\"/cc_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the `qq_df` table:\n",
    "\n",
    "- `ip`: index, IP address as string\n",
    "- `rank`: Alexa website rank\n",
    "- `site`: Website hostname\n",
    "- `ip6`: True if address is IPv6 (for convenience in splitting v4 and v6 analyses without string munging)\n",
    "- *vantage*`-`*trial*`-e0`: True if connection established for *trial* at *vantage* without ECN negotiation attempt\n",
    "- *vantage*`-`*trial*`-e0ect0`: True if non-ECN flow set ECT0 flag on downstream for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-e0ect1`: True if non-ECN flow set ECT1 flag on downstream for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-e0ce`: True if non-ECN flow set CE flag on downstream for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-e0f`: Flags on last SYN of upstream non-ECN flow for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-e0rf`: Flags on last SYN of downstream non-ECN flow for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-e0ruf`: Union flags on downstream non-ECN flow for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-ttl`: Max TTL received on downstream non-ECN flow for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-z0`: True if no payload received for non-ECN flow for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-z1`: True if no payload received for ECN flow for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-e1`: True if connection established for *trial* at *vantage* with ECN negotiation attempt\n",
    "- *vantage*`-`*trial*`-neg`: True if ECN negotiated for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-ect0`: True if ECN flow set ECT0 flag on downstream for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-ect1`: True if ECN flow set ECT1 flag on downstream for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-ce`: True if ECN flow set CE flag on downstream for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-synect0`: True if ECN flow set ECT0 flag on downstream SYN ACK for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-synect1`: True if ECN flow set ECT1 flag on downstream SYN ACK for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-synce`: True if ECN flow set CE flag on downstream SYN ACK for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-refl`: True if ECN TCP flags were reflected (SEW -> SAEW) for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-ect`: True if ECT0 or ECT1 set on downstream ECN flow for *trial* at *vantage*\n",
    "- *vantage*`-`*trial*`-negok`: True if negotiation succeeded (resulted in ECT marking) for  *trial* at *vantage*\n",
    "\n",
    "The `-sum` columns for each of the boolean columns above simply count True values across all trials and vantage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qq_df.to_csv(TABLESDIR+\"/qq_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
